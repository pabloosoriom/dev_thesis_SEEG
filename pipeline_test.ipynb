{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltools.data import Brain_Data, Design_Matrix, Adjacency\n",
    "import networkx as nx\n",
    "from scipy import signal\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ER(data, window_size, overlap):\n",
    "    \"\"\"\n",
    "    Calculate time-varying Energy Ratio (ER) from Theta(w) using a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "    - S: Periodogram estimate for Theta(w)\n",
    "    - f: Frequency vector\n",
    "    - window_size: Size of the sliding window in samples\n",
    "    - overlap: Overlap between consecutive windows in samples\n",
    "\n",
    "    Returns:\n",
    "    - time_points: Array of time points corresponding to the center of each window\n",
    "    - ER_values: Array of time-varying ER values\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty arrays to store results\n",
    "    time_points = []\n",
    "    ER_values = []\n",
    "\n",
    "    # Iterate through the signal with the sliding window\n",
    "    for start in range(0, len(data) - window_size + 1, overlap):\n",
    "        end = start + window_size\n",
    "        #(f, S)= signal.welch(data[start:end], fs=raw.info['sfreq'], nperseg=1024*5)\n",
    "        (f,S)=signal.periodogram(data[start:end],fs=raw.info['sfreq'],scaling='density')\n",
    "\n",
    "        # Calculate energy in each frequency band for the current window\n",
    "        ETheta = np.sum(S[np.where((f >= 3.5) & (f < 7.4))])\n",
    "        EAlpha = np.sum(S[np.where((f >= 7.4) & (f < 12.4))])\n",
    "        EBeta = np.sum(S[np.where((f >= 12.4) & (f < 24))])\n",
    "        EGamma = np.sum(S[np.where((f >= 24) & (f <= 97))])\n",
    "\n",
    "        # Calculate Energy Ratio (ER) for the current window\n",
    "        ER = (EBeta + EGamma) / (ETheta + EAlpha)\n",
    "\n",
    "        # Store results\n",
    "        time_points.append((start + end) / 2)  # Use the center of the window as the time point\n",
    "        ER_values.append(ER)\n",
    "\n",
    "    return np.array(time_points), np.array(ER_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_nihon('/home/pablo/Documents/Universidad Data/Maestría en Matemáticas Aplicadas/Tesis/data/FA330022.EEG', preload=True)\n",
    "raw.pick_types(eeg=True, bio=False, misc=False)\n",
    "\n",
    "#Dropping channels\n",
    "channels = raw.ch_names\n",
    "channels_to_remove=['E']\n",
    "raw.drop_channels(channels_to_remove)\n",
    "\n",
    "#Filter\n",
    "raw.filter(l_freq = 0, h_freq = 97.0)\n",
    "# Set the frequency you want to remove; it's commonly 50 Hz or 60 Hz\n",
    "notch_freq = 60  # or 60 for the USA and other countries using 60Hz\n",
    "# Apply notch filter\n",
    "raw.notch_filter(freqs = notch_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a matrix of U_n, where the rows are every channel and the columns are the windows\n",
    "#Getting the channels\n",
    "channels = raw.ch_names\n",
    "#Getting the number of channels\n",
    "n_channels=len(channels)\n",
    "#Getting the number of windows\n",
    "n_windows=len(U_n)\n",
    "#Making a matrix of zeros\n",
    "U_n_matrix=np.zeros((n_channels,n_windows))\n",
    "ER_matrix=np.zeros((n_channels,n_windows))\n",
    "ER_n_array=np.zeros(n_channels)\n",
    "#Alarm time array\n",
    "alarm_time=np.zeros(n_channels)\n",
    "#Detection time array\n",
    "#detection_time=np.zeros(n_channels)\n",
    "#A loop for every channel\n",
    "for k in range(n_channels):\n",
    "    #Getting the data of the channel\n",
    "    data=raw.get_data()[k]\n",
    "    #Getting the ER values\n",
    "    time_points, ER_values = calculate_ER(data, window_size, overlap)\n",
    "    ER=ER_values\n",
    "    #Normalizing between 0 and 1\n",
    "    ER=(ER-np.min(ER))/(np.max(ER)-np.min(ER))\n",
    "    ER_matrix[k,:]=ER\n",
    "    N=len(ER)\n",
    "    ER_n=(1/N)*np.sum(ER)\n",
    "    #Getting the ER_n values in the \n",
    "    ER_n_array[k]=ER_n\n",
    "    ##Getting U_n\n",
    "    U_n=np.zeros(len(ER))\n",
    "    v=0.1\n",
    "    u_min=0\n",
    "    #lambda_=108867\n",
    "    lambda_=125\n",
    "    alarm_times=[]\n",
    "    for i in range(N):\n",
    "        U_n[i]=np.sum(ER[0:i]-ER_n-0.1)\n",
    "        u_min=np.min(U_n)\n",
    "        if (U_n[i]-u_min)>lambda_:\n",
    "            #print('Anomaly detected at window number ',i, ' for channel ',k)    \n",
    "            alarm_times.append(i)\n",
    "            u_min=0\n",
    "            U_n[i]=0\n",
    "    #Saving the U_n values in the matrix\n",
    "    U_n_matrix[k,:]=U_n\n",
    "    #Getting the alarm time\n",
    "    alarm_time[k]=alarm_times[0]\n",
    "    \n",
    "#Getting EI\n",
    "N0=np.min(alarm_time)\n",
    "Ei=[]\n",
    "tau=1\n",
    "#H variable is equal to 5 seconds, so \n",
    "H=5*fs\n",
    "#sum from detection time to the end of the signal\n",
    "for k in range(n_channels):\n",
    "    Ei.append(((1/(alarm_time[k]-N0+tau))*np.sum(ER_matrix[k,int(alarm_time[k]):int(alarm_time[k]+H)])))\n",
    "\n",
    "\n",
    "#Plotting the U_n values for every in a heatmap with an x axis of the window number and a y axis of the channel name with imshow variable \"channels\" as the labels\n",
    "plt.imshow(ER_matrix,cmap='viridis',interpolation='bicubic',aspect='auto',extent=[0,40000,0,22])\n",
    "#colorbar\n",
    "plt.colorbar()\n",
    "plt.yticks(np.arange(len(channels)), channels)\n",
    "plt.xlabel('Window number')\n",
    "plt.ylabel('Channel name')\n",
    "plt.title('ER_n')\n",
    "plt.show()\n",
    "\n",
    "#Plting a barplt of the EI values for every channel\n",
    "Ei_n=Ei/np.max(Ei)\n",
    "plt.bar(channels,Ei)\n",
    "plt.xlabel('Channel name')\n",
    "plt.ylabel('EI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making 3 slided windows of 200 seconds with 25% of overlap\n",
    "epochs = mne.make_fixed_length_epochs(raw, duration=100, overlap=0.25,preload=True)\n",
    "times=epochs.times\n",
    "ch_names=epochs.ch_names\n",
    "\n",
    "fmin, fmax = 4., 9.  # compute connectivity within 4-9 Hz\n",
    "sfreq = raw.info['sfreq']  # sampling frequency\n",
    "tmin = 0.0  # exclude the baseline period\n",
    "\n",
    "# Compute PLI, wPLI, and dPLI\n",
    "con_pli = spectral_connectivity_epochs(\n",
    "    epochs, method='pli', mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
    "    fmax=fmax, faverage=True, tmin=tmin, mt_adaptive=False, n_jobs=1)\n",
    "\n",
    "con_wpli = spectral_connectivity_epochs(\n",
    "    epochs, method='wpli', mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
    "    fmax=fmax, faverage=True, tmin=tmin, mt_adaptive=False, n_jobs=1)\n",
    "\n",
    "con_dpli = spectral_connectivity_epochs(\n",
    "    epochs, method='dpli', mode='multitaper', sfreq=sfreq, fmin=fmin,\n",
    "    fmax=fmax, faverage=True, tmin=tmin, mt_adaptive=False, n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 5), sharey=True)\n",
    "axs[0].imshow(con_pli.get_data('dense'), vmin=0, vmax=1)\n",
    "axs[0].set_title(\"PLI\")\n",
    "axs[0].set_ylabel(\"Node 1\")\n",
    "axs[0].set_xlabel(\"Node 2\")\n",
    "\n",
    "axs[1].imshow(con_wpli.get_data('dense'), vmin=0, vmax=1)\n",
    "axs[1].set_title(\"wPLI\")\n",
    "axs[1].set_xlabel(\"Node 2\")\n",
    "\n",
    "im = axs[2].imshow(con_dpli.get_data('dense'), vmin=0, vmax=1)\n",
    "axs[2].set_title(\"dPLI\")\n",
    "axs[2].set_xlabel(\"Node 2\")\n",
    "\n",
    "fig.colorbar(im, ax=axs.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin = (8., 13.)\n",
    "fmax = (13., 30.)\n",
    "tmin = 0.0 \n",
    "sfreq = raw.info['sfreq']  # the sampling frequency\n",
    "\n",
    "coh = spectral_connectivity_epochs(\n",
    "    epochs, method='coh', mode='fourier', sfreq=sfreq, fmin=fmin,\n",
    "    fmax=fmax, faverage=True, tmin=tmin, mt_adaptive=False, n_jobs=1)\n",
    "freqs = coh.freqs\n",
    "\n",
    "print('Frequencies in Hz over which coherence was averaged for alpha: ')\n",
    "print(freqs[0])\n",
    "print('Frequencies in Hz over which coherence was averaged for beta: ')\n",
    "print(freqs[1])\n",
    "\n",
    "\n",
    "#Plotting coherence matrix \n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "axs[0].imshow(coh.get_data('dense')[:,:,0], vmin=0, vmax=1)\n",
    "axs[0].set_title(\"Alpha\")\n",
    "axs[0].set_ylabel(\"Node 1\")\n",
    "axs[0].set_xlabel(\"Node 2\")\n",
    "\n",
    "im = axs[1].imshow(coh.get_data('dense')[:,:,1], vmin=0, vmax=1)\n",
    "axs[1].set_title(\"Beta\")\n",
    "axs[1].set_xlabel(\"Node 2\")\n",
    "\n",
    "fig.colorbar(im, ax=axs.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation for graph connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data of every channel in a list of lists\n",
    "data=[]\n",
    "for i in range(0, len(chanels)):\n",
    "    data.append(raw.get_data(picks=chanels[i]))\n",
    "\n",
    "#Plotting the data of every channel in the same plot\n",
    "for i in range(0, 15):\n",
    "    plt.plot(data[i][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measuring connectivity between channels\n",
    "#Pearson correlation\n",
    "chanels=raw.ch_names\n",
    "corr=[]\n",
    "for i in range(0, len(chanels)):\n",
    "    for j in range(0, len(chanels)):\n",
    "        corr.append(np.corrcoef(data[i][0], data[j][0])[0][1])\n",
    "\n",
    "#Plotting the correlation matrix\n",
    "corr=np.array(corr)\n",
    "corr=corr.reshape(len(chanels), len(chanels))\n",
    "sns.heatmap(corr, square=True, vmin=-1, vmax=1, cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create a binary matrix, we use an arbitrary threshold in the correlation \n",
    "#matrix\n",
    "a = Adjacency(corr, matrix_type='similarity', labels=[x for x in chanels])\n",
    "a_thresholded = a.threshold(upper=.5, binarize=True)\n",
    "\n",
    "a_thresholded.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "G = a_thresholded.to_graph()\n",
    "pos = nx.kamada_kawai_layout(G)\n",
    "node_and_degree = G.degree()\n",
    "nx.draw_networkx_edges(G, pos, width=3, alpha=.2)\n",
    "nx.draw_networkx_labels(G, pos, font_size=14, font_color='darkslategray')\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=list(dict(node_and_degree).keys()),\n",
    "                       node_size=[x[1]*100 for x in node_and_degree],\n",
    "                       node_color=list(dict(node_and_degree).values()),\n",
    "                       cmap=plt.cm.Reds_r, linewidths=2, edgecolors='darkslategray', alpha=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dict(G.degree).values(), bins=20, color='lightseagreen', alpha=0.7)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Degree', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Degree per channel plot\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.barh(list(dict(G.degree).keys()), list(dict(G.degree).values()), color='lightseagreen')\n",
    "plt.xlabel('Degree', fontsize=18)\n",
    "plt.ylabel('Channel', fontsize=18)\n",
    "plt.title('Degree per channel', fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
