{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltools.data import Brain_Data, Design_Matrix, Adjacency\n",
    "import networkx as nx\n",
    "from scipy import signal\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "from itertools import cycle\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import plotly.graph_objs as go\n",
    "from functools import reduce\n",
    "import teneto\n",
    "from teneto import TemporalNetwork\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from functions.Connectivity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient='sub-HUP185'\n",
    "raw=mne.io.read_raw_fif(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_filtered.fif', preload=True)\n",
    "#Reading a npy file \n",
    "data_alpha = np.load(f'//home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_alpha_aec_dense.npy')\n",
    "data_beta = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_beta_aec_dense.npy')\n",
    "data_hgamma = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_high_gamma1_aec_dense.npy')\n",
    "data_lgamma = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_low_gamma_aec_dense.npy')\n",
    "data_theta = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_theta_aec_dense.npy')\n",
    "\n",
    "data_alpha_norm = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_alpha_aec_distance_dense.npy')\n",
    "data_beta_norm = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_beta_aec_distance_dense.npy')\n",
    "\n",
    "data_hgamma_norm = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_high_gamma1_aec_distance_dense.npy')\n",
    "data_lgamma_norm = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_low_gamma_aec_distance_dense.npy')\n",
    "data_theta_norm = np.load(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/{patient}_connectivity_data_theta_aec_distance_dense.npy')\n",
    "# Reading the .tsv file \n",
    "xyz_loc = pd.read_csv(f'/home/pablo/works/dev_thesis_SEEG/outputs_t03/{patient}/ref_0/percentile_0.9/xyz_loc.csv', sep='\\t')\n",
    "\n",
    "# #Data with other metrics\n",
    "# data_coh = np.load('/home/pablo/works/dev_thesis_SEEG/outputs/pte_01/SR_subseg_connectivity_data_high_freq_coh_dense.npy')[:,:,:,0]\n",
    "# # Assuming data is the array with shape (50, 127, 127)\n",
    "# for i in range(data_coh.shape[0]):\n",
    "#     # Extract the lower triangular part (including the diagonal)\n",
    "#     lower_triangular = np.tril(data_coh[i])\n",
    "    \n",
    "#     # Mirror the lower triangular part to the upper triangular part\n",
    "#     data_coh[i] = lower_triangular + lower_triangular.T - np.diag(np.diag(lower_triangular))\n",
    "\n",
    "# #Make data_coh symmetric\n",
    "# #To know the index of the channels [\"lp'11\", \"lp'12\", \"op'12\", \"pi'18\", \"pa'12\"] in the raw object into an array\n",
    "# idx_channels = [raw.ch_names.index(ch) for ch in [\"lp'11\", \"lp'12\", \"op'12\", \"pi'18\", \"pa'12\"]]\n",
    "# #Eliminate from data_coh the channels that are not in the raw object\n",
    "# data_coh = np.delete(data_coh, idx_channels, axis=1)\n",
    "# data_coh = np.delete(data_coh, idx_channels, axis=2)\n",
    "# # con_data=np.mean(data,axis=3)\n",
    "# raw.drop_channels([\"lp'11\", \"lp'12\", \"op'12\", \"pi'18\", \"pa'12\"])\n",
    "\n",
    "\n",
    "# inside_channels=['RA01','RA02','RA03','RA04','RB01','RB02','RB03','RB04','RC01','RC02','RC03','RC04','RC05']\n",
    "inside_channels=['LA01','LA02','LA03','LA04','LA05','LB01','LB02','LB03','LB04']\n",
    "# #Get the channels outside the SOZ \n",
    "# outside_channels=[ch for ch in raw.ch_names if ch not in inside_channels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_lgamma\n",
    "data=data.transpose(1,2,0)\n",
    "percent=0.9\n",
    "\n",
    "tnet_bu= TemporalNetwork(N=data.shape[0],T=data.shape[2],nettype='wu',from_array=data,\n",
    "                      timetype='discrete',timeunit='epoch', nodelabels=list(xyz_loc['formatted_label'].values))\n",
    "\n",
    "tnet_bu.binarize(threshold_type='percent',threshold_level=1-percent,axis='graphlet')\n",
    "\n",
    "tnet_wu=TemporalNetwork(N=data.shape[0],T=data.shape[2],nettype='wu',from_array=data,\n",
    "                      timetype='discrete',timeunit='epoch', nodelabels=list(xyz_loc['formatted_label'].values))\n",
    "\n",
    "tnet_bu_ar=tnet_bu.network\n",
    "communities_dict={}\n",
    "k=4\n",
    "algorithm='k_clique_communities'\n",
    "\n",
    "for i in range(tnet_bu_ar.shape[2]):\n",
    "    #Building a network using adjacency matrix\n",
    "    adj = Adjacency(tnet_bu_ar[:,:,i],labels=raw.ch_names)\n",
    "    \n",
    "    G = nx.Graph(adj.to_graph())\n",
    "    #Community detection algorithm \n",
    "    if algorithm=='girvan_newman':\n",
    "        communities_generator=nx.community.girvan_newman(G)\n",
    "        next_level_communities = next(communities_generator)\n",
    "        communities = sorted(map(sorted, next_level_communities))\n",
    "    elif algorithm=='edge_current_flow_betweenness_partition':\n",
    "        communities_generator=nx.community.edge_current_flow_betweenness_partition(G, number_of_sets=k)\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='edge_betweenness_partition':\n",
    "        communities_generator=nx.community.edge_betweenness_partition(G, number_of_sets=k)\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='k_clique_communities':\n",
    "        communities_generator=list(nx.community.k_clique_communities(G,k))\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='greedy_modularity_communities':\n",
    "        communities_generator=nx.community.greedy_modularity_communities(G)\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='naive_greedy_modularity_communities': \n",
    "        communities_generator=nx.community.naive_greedy_modularity_communities(G)\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='fast_label_propagation_communities':\n",
    "        communities_generator=nx.community.fast_label_propagation_communities(G)\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='louvain_communities':\n",
    "        communities_generator=nx.community.louvain_communities(G)\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='asyn_fluidc':\n",
    "        #this algorithm only works for undirected and completely connected graphs\n",
    "        communities_generator=nx.community.asyn_fluidc(G,k)\n",
    "        communities=[list(c) for c in communities_generator]\n",
    "    elif algorithm=='kernighan_lin_bisection':\n",
    "        communities = [list(nx.community.kernighan_lin_bisection(G)[0])]\n",
    "        communities.append(list(nx.community.kernighan_lin_bisection(G)[1]))\n",
    "    communities_dict[i]=communities\n",
    "\n",
    "# Number of time steps\n",
    "num_timesteps = len(communities_dict)\n",
    "\n",
    "# Number of channels\n",
    "num_channels = len(raw.ch_names)\n",
    "\n",
    "# Initialize an array for each time step (2D list) with default value -1 (indicating no community yet)\n",
    "output = [[-1 for _ in range(num_timesteps)] for _ in range(num_channels)]\n",
    "\n",
    "# Iterate through the communities dictionary\n",
    "for t, subcommunities in communities_dict.items():\n",
    "    # Loop through each subcommunity and assign its index to the respective channels\n",
    "    for subcom_idx, subcommunity in enumerate(subcommunities):\n",
    "        for channel in subcommunity:\n",
    "            # Find the index of the channel in ch_names\n",
    "            if channel in raw.ch_names:\n",
    "                channel_idx = raw.ch_names.index(channel)\n",
    "                # Assign the subcommunity index to the correct channel\n",
    "                output[channel_idx][t] = subcom_idx\n",
    "\n",
    "#Making temporal consensus\n",
    "communities_after=teneto.communitydetection.make_temporal_consensus(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the communities\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 20))\n",
    "\n",
    "# Plot for 'Community assignment over time before temporal consensus'\n",
    "im1 = axes[0].imshow(output, aspect='auto', cmap='tab20')\n",
    "axes[0].set_xlabel('Time step')\n",
    "axes[0].set_ylabel('Channel index')\n",
    "axes[0].set_title('Community assignment over time before temporal consensus')\n",
    "axes[0].set_yticks(np.arange(0, len(raw.ch_names), 1))\n",
    "axes[0].set_yticklabels(raw.ch_names)\n",
    "fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot for 'Community assignment over time after temporal consensus'\n",
    "im2 = axes[1].imshow(communities_after, aspect='auto', cmap='tab20')\n",
    "axes[1].set_xlabel('Time step')\n",
    "axes[1].set_ylabel('Channel index')\n",
    "axes[1].set_title('Community assignment over time after temporal consensus')\n",
    "axes[1].set_yticks(np.arange(0, len(raw.ch_names), 1))\n",
    "axes[1].set_yticklabels(raw.ch_names)\n",
    "fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.suptitle(f'Community detection using {algorithm} \\n')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allegiance_m=teneto.communitymeasures.allegiance(communities_after)\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(allegiance_m,cmap='viridis')\n",
    "plt.title('Allegiance')\n",
    "plt.xlabel('Channels')\n",
    "plt.ylabel('Communities')\n",
    "# plt.yticks(np.arange(0, len(xyz_loc['formatted_label'].values), 1), xyz_loc['formatted_label'].values)\n",
    "# plt.xticks(np.arange(0, len(xyz_loc['formatted_label'].values), 1), xyz_loc['formatted_label'].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rebuilding the communities for each time step\n",
    "communities_dict_after={}\n",
    "for t in range(len(communities_after[0])):\n",
    "    temp_com={}\n",
    "    for i, ch in enumerate(raw.ch_names):\n",
    "        temp_com[communities_after[i][t]]=temp_com.get(communities_after[i][t],[])+[ch]\n",
    "    communities_dict_after[t]=temp_com       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_dict_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE are going to extrac the Jaccard index for each community in each time step\n",
    "#Comparing with the inside channels\n",
    "jaccard_index={}\n",
    "for t in range(len(communities_dict_after)):\n",
    "    temp_jaccard={}\n",
    "    for com in communities_dict_after[t]:\n",
    "        temp_jaccard[com]=jaccard_similarity(inside_channels,communities_dict_after[t][com])\n",
    "    jaccard_index[t]=temp_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the maximum for each time step, and the community channels that belong to it \n",
    "max_jaccard=[]\n",
    "max_jaccard_com=[]\n",
    "for t in range(len(jaccard_index)):\n",
    "    max_jaccard.append(max(jaccard_index[t].values()))\n",
    "    max_jaccard_com.append(max(jaccard_index[t], key=jaccard_index[t].get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting max_jaccard   \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(max_jaccard)\n",
    "plt.title('Jaccard index')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Jaccard index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the communities of maximum jaccard index\n",
    "dict_communities={}\n",
    "for t in range(len(communities_dict_after)):\n",
    "    print(f'Time step {t}: {communities_dict_after[t][max_jaccard_com[t]]}')\n",
    "    dict_communities[t]=communities_dict_after[t][max_jaccard_com[t]]\n",
    "\n",
    "print('The inside channels are:',inside_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other similarity metrics\n",
    "# Overlap \n",
    "overlap_index={}\n",
    "for t in range(len(communities_dict_after)):\n",
    "    temp_overlap={}\n",
    "    for com in communities_dict_after[t]:\n",
    "        temp_overlap[com]=len(set(inside_channels).intersection(communities_dict_after[t][com])) / max(len(inside_channels),len(communities_dict_after[t][com]))\n",
    "    overlap_index[t]=temp_overlap\n",
    "\n",
    "#Get the maximum for each time step, and the community channels that belong to it\n",
    "max_overlap=[]\n",
    "max_overlap_com=[]\n",
    "for t in range(len(overlap_index)):\n",
    "    max_overlap.append(max(overlap_index[t].values()))\n",
    "    max_overlap_com.append(max(overlap_index[t], key=overlap_index[t].get))\n",
    "\n",
    "#Plotting max_overlap\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(max_overlap)\n",
    "plt.title('Overlap index')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Overlap index')\n",
    "plt.show()\n",
    "\n",
    "#printing the communities of maximum overlap index\n",
    "\n",
    "for t in range(len(communities_dict_after)):\n",
    "    print(f'Time step {t}: {communities_dict_after[t][max_overlap_com[t]]}')\n",
    "\n",
    "print('The inside channels are:',inside_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For making the method even more unsupervised, we are going to analyze the communities generated at every timestep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = Adjacency(tnet_bu_ar[:,:,2],labels=raw.ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_2=communities_dict_after[2]\n",
    "#Extract the adjacency matrix for each community\n",
    "adj_com={}\n",
    "for com, ch in com_2.items():\n",
    "    com_idx=[raw.ch_names.index(chh) for chh in ch]\n",
    "    adj_com[com]=Adjacency(tnet_bu_ar[com_idx][:,com_idx,2],labels=ch)\n",
    "\n",
    "#Make it a graph\n",
    "G = nx.Graph(adj_com[2].to_graph())\n",
    "#Compute the eigenvector centrality and the betweenness centrality\n",
    "eigenvector_centrality = nx.katz_centrality_numpy(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_dict_after.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.radius(nx.Graph(adj_com[1].to_graph()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Function to calculate conductance of a community\n",
    "def conductance(G, community):\n",
    "    \"\"\"\n",
    "    Calculate the conductance of a single community in a graph G.\n",
    "\n",
    "    Parameters:\n",
    "    - G: The graph (networkx.Graph)\n",
    "    - community: A set of nodes representing the community\n",
    "\n",
    "    Returns:\n",
    "    - conductance_score: The conductance of the community\n",
    "    \"\"\"\n",
    "    # Convert the community set to a list\n",
    "    community = set(community)\n",
    "    \n",
    "    # Compute the number of edges between the community and the rest of the graph (cut size)\n",
    "    cut_size = nx.cut_size(G, community)\n",
    "    print('The cut size is:',cut_size)\n",
    "    \n",
    "    # Compute the volume of the community (sum of degrees of the nodes inside the community)\n",
    "    community_volume = sum(G.degree(n) for n in community)\n",
    "    print('The community volume is:',community_volume)\n",
    "    \n",
    "    # Compute the volume of the rest of the graph\n",
    "    outside_volume = sum(G.degree(n) for n in set(G.nodes()) - community)\n",
    "    print('The outside volume is:',outside_volume)\n",
    "    \n",
    "    # Calculate the conductance score\n",
    "    conductance_score = cut_size / (min(community_volume, outside_volume))\n",
    "    print('The conductance score is:',conductance_score)\n",
    "    \n",
    "    return conductance_score\n",
    "\n",
    "def normalized_cut(G, community):\n",
    "    \"\"\"\n",
    "    Calculate the normalized cut for a single community in a graph G.\n",
    "\n",
    "    Parameters:\n",
    "    - G: The graph (networkx.Graph)\n",
    "    - community: A set of nodes representing the community\n",
    "\n",
    "    Returns:\n",
    "    - normalized_cut_score: The normalized cut value of the community\n",
    "    \"\"\"\n",
    "    # Convert the community set to a set for better performance\n",
    "    community = set(community)\n",
    "    \n",
    "    # Compute the number of edges between the community and the rest of the graph (cut size)\n",
    "    cut_size = nx.cut_size(G, community)\n",
    "    \n",
    "    # Compute the volume of the community (sum of degrees of nodes inside the community)\n",
    "    community_volume = sum(G.degree(n) for n in community)\n",
    "    \n",
    "    # Compute the volume of the rest of the graph (sum of degrees of nodes outside the community)\n",
    "    outside_community = set(G.nodes()) - community\n",
    "    outside_volume = sum(G.degree(n) for n in outside_community)\n",
    "    \n",
    "    # Avoid division by zero by checking volumes\n",
    "    if community_volume == 0 or outside_volume == 0:\n",
    "        return float('inf')  # Return a large value if one of the volumes is zero (invalid cut)\n",
    "    \n",
    "    # Calculate normalized cut\n",
    "    normalized_cut_score = (cut_size / community_volume) + (cut_size / outside_volume)\n",
    "    \n",
    "    return normalized_cut_score\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "def intra_community_density_with_size_regularization(G, community, alpha=0.6, size_threshold=15):\n",
    "    \"\"\"\n",
    "    Calculate the intra-community density with size regularization for a community in a graph.\n",
    "\n",
    "    Parameters:\n",
    "    - G: The graph (networkx.Graph)\n",
    "    - community: A set of nodes representing the community\n",
    "    - alpha: Regularization factor to penalize large or small communities (default: 0.1)\n",
    "    - size_threshold: The ideal size of the community (default: 10)\n",
    "\n",
    "    Returns:\n",
    "    - density_score: The intra-community density with regularization\n",
    "    \"\"\"\n",
    "    community = set(community)\n",
    "    num_nodes = len(community)\n",
    "    \n",
    "    if num_nodes <= 3:\n",
    "        return 0  # If the community is too small, return density as 0\n",
    "\n",
    "    # Count edges within the community\n",
    "    intra_edges = G.subgraph(community).number_of_edges()\n",
    "\n",
    "    # Calculate intra-community density\n",
    "    possible_edges = num_nodes * (num_nodes - 1) / 2\n",
    "    if possible_edges == 0:\n",
    "        density = 0\n",
    "    else:\n",
    "        density = intra_edges / possible_edges\n",
    "    \n",
    "    # Size regularization: penalize communities that are too small or too large\n",
    "    size_penalty = alpha * abs(num_nodes - size_threshold) / size_threshold\n",
    "\n",
    "    # Regularized density score\n",
    "    density_score = density - size_penalty\n",
    "    \n",
    "    return max(0, density_score)  # Ensure non-negative density score\n",
    "\n",
    "# t=4\n",
    "# com_2=communities_dict_after[t]\n",
    "\n",
    "# # Function to calculate conductance of all communities\n",
    "# adj = Adjacency(tnet_bu_ar[:,:,t],labels=raw.ch_names)\n",
    "# G = nx.Graph(adj.to_graph())\n",
    "\n",
    "# conductance_scores = {}\n",
    "# for com, ch in com_2.items():\n",
    "#     # com_idx=[raw.ch_names.index(chh) for chh in ch]\n",
    "#     # adj_com=Adjacency(tnet_bu_ar[com_idx][:,com_idx,t],labels=ch)\n",
    "#     # G = nx.Graph(adj_com.to_graph())\n",
    "#     print('The community is:',ch)\n",
    "#     conductance_scores[com] = intra_community_density_with_size_regularization(G, ch)\n",
    "\n",
    "# print(conductance_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Final communities for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get for every time step the communities that have the maximum intra_community_density_with_size_regularization\n",
    "max_density_scores=[]\n",
    "max_density_scores_com=[]\n",
    "best_communities={}\n",
    "communities_tuple=[]\n",
    "for t in range(len(communities_dict_after)):\n",
    "    com_t=communities_dict_after[t]\n",
    "    adj = Adjacency(tnet_bu_ar[:,:,t],labels=raw.ch_names)\n",
    "    G = nx.Graph(adj.to_graph())\n",
    "    density_scores = {}\n",
    "    commun=[]\n",
    "    for com, ch in com_t.items():\n",
    "        density_scores[com] = intra_community_density_with_size_regularization(G, ch)\n",
    "        commun.append(ch)\n",
    "    max_density_scores.append(max(density_scores.values()))\n",
    "    #Get the community with the maximum density score\n",
    "    best_communities[t]=commun[list(density_scores.values()).index(max(density_scores.values()))]\n",
    "    max_density_scores_com.append(max(density_scores, key=density_scores.get))\n",
    "    communities_tuple.append((best_communities[t],max_density_scores[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_communities(data_dict):\n",
    "    final_array = []\n",
    "\n",
    "    # Iterate through each time step (t)\n",
    "    for t in range(len(next(iter(data_dict.values())))):  # Assuming all algorithms have the same time steps\n",
    "        max_density = float('-inf')\n",
    "        selected_algorithm = None\n",
    "        selected_community = None\n",
    "\n",
    "        # Loop through algorithms to find the max density at time t\n",
    "        for algorithm, communities in data_dict.items():\n",
    "            community, density = communities[t]\n",
    "\n",
    "            # Update the best algorithm-community pair for this time step\n",
    "            if density > max_density:\n",
    "                max_density = density\n",
    "                selected_algorithm = algorithm\n",
    "                selected_community = community\n",
    "\n",
    "        # Apply the conditional: if the max density is 0, return an empty community\n",
    "        if max_density == 0:\n",
    "            selected_community = []\n",
    "\n",
    "        # Store the result for this time step\n",
    "        final_array.append((selected_algorithm, [selected_community], max_density))\n",
    "\n",
    "    return final_array\n",
    "\n",
    "result = get_max_communities({'greedy_modularity': communities_tuple})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_communities={}\n",
    "#Get the t and the communities with a density value different from 0\n",
    "for t in range(len(best_communities)):\n",
    "    if max_density_scores[t]!=0:\n",
    "        final_communities[t]=best_communities[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_final_communities(final_communities,comparision_set):\n",
    "    jaccard_index={}\n",
    "    for t, com in final_communities.items():\n",
    "        jaccard_index[t]=jaccard_similarity(comparision_set,com)\n",
    "\n",
    "    return jaccard_index\n",
    "\n",
    "# jaccard_final_communities(final_communities,inside_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/pablo/works/dev_thesis_SEEG/outputs/pte_01/'\n",
    "\n",
    "with open(output_path+'final_com_high_gamma1_aec_distance_.json') as f:\n",
    "        communities_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a plot of the density overtime \n",
    "density_scores = [density for _, _, density in communities_data]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(density_scores)\n",
    "plt.title('Max Intra-community density with size regularization over time')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Density score')\n",
    "plt.show()\n",
    "\n",
    "#Getting the Jaccard index for the final communities over time\n",
    "inside_network = [\"m'3\",\"sc'3\",\"sc'4\",\"sc'5\",\"sc'6\",\"y'4\",\"y'5\",\"y'6\",\"y'7\",\"y'8\",\"y'9\"]\n",
    "\n",
    "jaccard=[]\n",
    "for _, community, _ in communities_data:\n",
    "    jaccard.append(jaccard_similarity(inside_network,community[0]))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(jaccard)\n",
    "plt.title('Jaccard index')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Jaccard index')\n",
    "plt.show()\n",
    "# communities = [community for _, community, _ in communities_data]\n",
    "# jaccard_scores = jaccard_final_communities(communities,inside_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading json file\n",
    "\n",
    "with open('/home/pablo/works/dev_thesis_SEEG/outputs/sub-HUP142/ref_0/final_com_high_gamma1_aec_.json') as f:\n",
    "        final_communities_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example list\n",
    "communities = final_communities_data\n",
    "\n",
    "# \"Real contacts\" provided in an array\n",
    "inside_network = set(['LDA1','LDA2','LDA3','LDA4','LDA5','LDA6','LDA7','LDB1','LDB2','LDB3','LDB4','LDB5','LDB6','LDC2','LDC3'])  # Replace with actual data\n",
    "\n",
    "# Step 1: Assign cumulative densities\n",
    "contact_densities = {}\n",
    "for community_type, community_list, density in communities:\n",
    "    for community in community_list:\n",
    "        for contact in community:\n",
    "            if contact not in contact_densities:\n",
    "                contact_densities[contact] = 0\n",
    "            contact_densities[contact] += density\n",
    "\n",
    "# Step 2: Normalize cumulative densities\n",
    "max_density = max(contact_densities.values())\n",
    "min_density = min(contact_densities.values())\n",
    "normalized_densities = {k: (v - min_density) / (max_density - min_density) for k, v in contact_densities.items()}\n",
    "\n",
    "# Step 3: Prepare for AUC calculation\n",
    "y_true = np.array([1 if contact in inside_network else 0 for contact in normalized_densities.keys()])\n",
    "y_scores = np.array([score for score in normalized_densities.values()])\n",
    "\n",
    "# Step 4: Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "# Step 5: Plot the AUC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All densities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Channels\n",
    "ref_data=0\n",
    "patient='sub-HUP144'\n",
    "\n",
    "main_path='/home/pablo/works/dev_thesis_SEEG/data/mainDatabase_patients/'\n",
    "doc_file_data=pd.read_csv(main_path+'/'+patient+'/'+'ses-presurgery/'+patient+'_ses-presurgery_scans.tsv',sep='\\t')\n",
    "\n",
    "\n",
    "xyz_loc=pd.read_csv(main_path+'/'+patient+'/'+'ses-presurgery/ieeg/'+patient+'_ses-presurgery_acq-seeg_space-fsaverage_electrodes.tsv',sep='\\t')\n",
    "events=pd.read_csv(main_path+'/'+patient+'/'+'ses-presurgery/'+doc_file_data['filename'][ref_data].replace('_ieeg.edf','_events.tsv'),sep='\\t')\n",
    "channels=pd.read_csv(main_path+'/'+patient+'/'+'ses-presurgery/'+doc_file_data['filename'][ref_data].replace('_ieeg.edf','_channels.tsv'),sep='\\t')\n",
    "\n",
    "\n",
    "good_channels=channels[channels['status']=='good']\n",
    "inside_network=list(good_channels[(good_channels['status_description'] == 'resect') |\n",
    "    (good_channels['status_description'] == 'soz') |\n",
    "    (good_channels['status_description'] == 'resect,soz') |\n",
    "    (good_channels['status_description'] == 'soz,resect')\n",
    "]['name'])\n",
    "\n",
    "\n",
    "with open(f'/home/pablo/works/dev_thesis_SEEG/outputs/{patient}/ref_{ref_data}/Tuples_com_low_gamma_aec_.json') as f:\n",
    "        final_communities_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example list\n",
    "communities = final_communities_data\n",
    "\n",
    "# \"Real contacts\" provided in an \n",
    "# Step 1: Assign cumulative densities\n",
    "contact_densities = {}\n",
    "for algorithm in communities:\n",
    "    communities_t=communities[algorithm]\n",
    "    for community_list, density in communities_t:\n",
    "        for contact in community_list:\n",
    "            if contact not in contact_densities:\n",
    "                contact_densities[contact] = 0\n",
    "            contact_densities[contact] += density\n",
    "\n",
    "# Step 2: Normalize cumulative densities\n",
    "max_density = max(contact_densities.values())\n",
    "min_density = min(contact_densities.values())\n",
    "normalized_densities = {k: (v - min_density) / (max_density - min_density) for k, v in contact_densities.items()}\n",
    "\n",
    "# Step 3: Prepare for AUC calculation\n",
    "y_true = np.array([1 if contact in inside_network else 0 for contact in normalized_densities.keys()])\n",
    "y_scores = np.array([score for score in normalized_densities.values()])\n",
    "\n",
    "# Step 4: Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "# Step 5: Plot the AUC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From dict to pd \n",
    "import pandas as pd\n",
    "df=pd.DataFrame.from_dict(normalized_densities,orient='index',columns=['Normalized density'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Epilep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
